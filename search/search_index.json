{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"no/llm Standard Interface for Large Language Models <p><code>no/llm</code> is a Python library that provides a unified interface for working with LLMs, with built-in support for model configuration, parameter validation, and provider management.</p> <p>\u26a0\ufe0f Early Stage Development This project is in early stages and under active development. While we're working hard to maintain stability, APIs and features may change as we improve the library. We encourage you to try it out and provide feedback, but please be aware that production use should be carefully considered.</p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>uv pip install \"no_llm[pydantic-ai]\"\n</code></pre>"},{"location":"#quick-example-with-pydantic-ai","title":"Quick Example with Pydantic AI","text":"<p>Free Testing</p> <p>Get a free API key from OpenRouter to test various models without individual provider accounts.</p> <pre><code>import os\n\nfrom no_llm.integrations.pydantic_ai import NoLLMModel\nfrom no_llm.registry import ModelRegistry\nfrom no_llm.settings import ValidationMode, settings\nfrom pydantic_ai import Agent\nfrom pydantic_ai.settings import ModelSettings\n\nsettings.validation_mode = ValidationMode.ERROR\n# or ValidationMode.WARN, ValidationMode.CLAMP\n\nos.environ[\"OPENROUTER_API_KEY\"] = \"...\"\n\n# Get model from registry\nregistry = ModelRegistry()\nopenrouter_models = list(registry.list_models(provider=\"openrouter\"))\nprint([m.identity.id for m in openrouter_models])\n# &gt; ['claude-3.5-haiku', 'claude-3.5-sonnet-v2', 'claude-3.7-sonnet', 'deepseek-chat', 'deepseek-r1-llama-70b-distilled', 'deepseek-reasoner', ...]\nno_llm_model = NoLLMModel(*openrouter_models)\n\n# Use with Pydantic AI\nagent = Agent(no_llm_model, model_settings=ModelSettings(temperature=1.2))\nresult = agent.run_sync(\"What is the capital of France?\")\nprint(result.data)\n# &gt; 2025-04-09 09:50:51.375 | WARNING  | no_llm.integrations.pydantic_ai:request:220 - Model deepseek-chat failed, trying next fallback. Error: Invalid value for parameter 'temperature'\n# &gt; Current value: 1.2\n# &gt; Valid range: (0.0, 1.0)\n# &gt; Error: Value 1.2 outside range [0.0, 1.0]\n# &gt; 2025-04-09 09:50:51.375 | WARNING  | no_llm.integrations.pydantic_ai:request:220 - Model deepseek-r1-llama-70b-distilled failed, trying next fallback. Error: Invalid value for parameter 'temperature'\n# &gt; Current value: 1.2\n# &gt; Valid range: (0.0, 1.0)\n# &gt; Error: Value 1.2 outside range [0.0, 1.0]\n# \u2705 gemini-2.0-flash\n# &gt; The capital of France is **Paris**.\n</code></pre>"},{"location":"#why-nollm","title":"Why no/llm?","text":"<ul> <li> <p>Provider Agnostic: Support for OpenAI, Anthropic, Google, Mistral, Groq, and more through a single interface</p> </li> <li> <p>Built-in Validation: Type-safe parameter validation and capability checking</p> </li> <li> <p>Provider Fallbacks: Automatic fallback between providers and data centers</p> </li> <li> <p>Configuration System: YAML-based model configurations with inheritance support</p> </li> <li> <p>Model Registry: Central management of models with capability-based filtering</p> </li> <li> <p>Integration Ready: Works with Pydantic AI, and more frameworks coming soon</p> </li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>Parameter System</li> <li>Provider Documentation</li> <li>Registry System</li> </ul>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#basic-installation","title":"Basic Installation","text":"<p>Install no_llm using uv:</p> <pre><code>uv pip install no_llm\n</code></pre>"},{"location":"install/#optional-dependencies","title":"Optional Dependencies","text":"<p>no_llm provides optional integrations that can be installed with extras:</p> <pre><code># Install with Pydantic AI support\nuv pip install \"no_llm[pydantic-ai]\"\n</code></pre>"},{"location":"install/#development-installation","title":"Development Installation","text":"<p>For development, clone the repository and install in editable mode:</p> <pre><code>git clone https://github.com/Noxus-AI/no-llm\ncd no-llm\nuv pip install -e \".[pydantic-ai]\"\n</code></pre>"},{"location":"registry/","title":"Model Registry","text":"<p>The registry system provides a central interface for managing model configurations and loading them from files.</p>"},{"location":"registry/#basic-usage","title":"Basic Usage","text":"<pre><code>from no_llm.registry import ModelRegistry\nfrom no_llm.config.enums import ModelMode, ModelCapability\n\n# Initialize registry (optionally with config directory)\nregistry = ModelRegistry(\"configs/\")\n\n# Get a specific model\nmodel = registry.get_model(\"gpt-4\")\n\n# List models with filters\nchat_models = registry.list_models(mode=ModelMode.CHAT)\nstreaming_models = registry.list_models(capabilities={ModelCapability.STREAMING})\n</code></pre>"},{"location":"registry/#configuration-inheritance","title":"Configuration Inheritance","text":"<p>The registry supports merging custom configurations with built-in ones:</p> <pre><code># configs/models/gpt-4.yml\n# Inherits from built-in GPT-4 configuration\nidentity:\n  id: gpt-4  # Must match built-in model ID\n  description: \"Custom GPT-4 configuration\"  # Overrides built-in description\n\nproviders:\n  - type: azure  # Override provider\n    api_key: $AZURE_API_KEY\n    deployment: gpt4\n\nconstraints:\n  max_input_tokens: 6000  # Override specific constraint\n</code></pre> <pre><code># Load built-in models first\nregistry = ModelRegistry()\n\n# Then load custom configurations\n# Custom configs will merge with built-in ones\nregistry.register_models_from_directory(\"configs/models\")\n</code></pre> <p>Configuration Merging</p> <ul> <li>Custom configurations are merged with built-in ones based on model ID</li> <li>Custom values override built-in values</li> <li>Unspecified fields keep their built-in values</li> <li>This allows partial configuration overrides</li> </ul>"},{"location":"registry/#model-filtering","title":"Model Filtering","text":"<p>The registry supports flexible model filtering:</p> <pre><code>from no_llm.registry import SetFilter\n\n# Match ANY of the capabilities\nmodels = registry.list_models(\n    capabilities={ModelCapability.STREAMING, ModelCapability.VISION}\n)\n\n# Match ALL capabilities\nmodels = registry.list_models(\n    capabilities=SetFilter(\n        values={ModelCapability.STREAMING, ModelCapability.FUNCTION_CALLING},\n        mode=\"all\"\n    )\n)\n\n# Filter by multiple criteria\nmodels = registry.list_models(\n    mode=ModelMode.CHAT,\n    capabilities={ModelCapability.STREAMING},\n    privacy_levels={PrivacyLevel.BASIC}\n)\n</code></pre>"},{"location":"registry/#registry-management","title":"Registry Management","text":"<pre><code># Load from directory\nregistry = ModelRegistry(\"configs/\")\n\n# Reload all configurations\nregistry.reload_configurations()\n\n# Register single model\nregistry.register_model(model_config)\n\n# Remove model\nregistry.remove_model(\"gpt-4\")\n</code></pre> <p>See the Model Configuration documentation for details about configuration formats.</p>"},{"location":"configs/capabilities/","title":"Model Capabilities","text":"<p>Capabilities define what features a model supports, enhancing its core functionality defined by its mode.</p>"},{"location":"configs/capabilities/#available-capabilities","title":"Available Capabilities","text":"<p>Models can support various capabilities that enhance their functionality:</p> Capability Description <code>STREAMING</code> Ability to stream responses token by token <code>FUNCTION_CALLING</code> Support for function/tool calling in responses <code>PARALLEL_FUNCTION_CALLING</code> Ability to call multiple functions simultaneously <code>VISION</code> Support for processing image inputs <code>SYSTEM_PROMPT</code> Support for system-level prompting <code>TOOLS</code> Support for using external tools <code>JSON_MODE</code> Ability to output responses in JSON format <code>STRUCTURED_OUTPUT</code> Support for generating structured data outputs <code>REASONING</code> Advanced reasoning capabilities <code>WEB_SEARCH</code> Ability to perform web searches"},{"location":"configs/capabilities/#runtime-capability-validation","title":"Runtime Capability Validation","text":"<p>Capability checking is crucial for runtime validation before making API requests. Different models support different features, and attempting to use unsupported capabilities can result in errors or unexpected behavior. For example:</p> <ul> <li>OpenAI O1 models don't support system prompts</li> <li>Some models don't support function calling</li> <li>Vision capabilities are only available in specific model versions</li> </ul> <p>By checking capabilities before making requests, you can: - Prevent failed API calls - Provide better error messages - Adapt your application's behavior based on available features - Handle model upgrades gracefully</p>"},{"location":"configs/capabilities/#filtering-models-by-capability","title":"Filtering Models by Capability","text":"<p>You can list models that match specific capability requirements using the registry:</p> <pre><code>from no_llm.config.enums import ModelCapability\nfrom no_llm.registry import ModelRegistry, SetFilter\n\nregistry = ModelRegistry()\n\n# List models with any of the specified capabilities\nstreaming_or_json_models = list(registry.list_models(\n    capabilities={ModelCapability.STREAMING, ModelCapability.JSON_MODE}\n))\n\n# List models that have ALL specified capabilities\nadvanced_models = list(registry.list_models(\n    capabilities=SetFilter(\n        values={ModelCapability.STREAMING, ModelCapability.FUNCTION_CALLING, ModelCapability.JSON_MODE},\n        mode=\"all\"\n    )\n))\n</code></pre>"},{"location":"configs/capabilities/#parameter-validation","title":"Parameter Validation","text":"<p>Parameters are validated against model capabilities:</p> <pre><code>from no_llm.config.model import ModelConfiguration\nfrom no_llm.config.enums import ModelCapability\n\nmodel_config = registry.get_model(\"gpt-4\")\n\n# Parameters will be validated against available capabilities\nparameters = model_config.parameters\nparameters.set_parameters(\n    capabilities={ModelCapability.STREAMING, ModelCapability.FUNCTION_CALLING},\n    temperature=0.7,\n    include_reasoning=True  # This will be dropped if REASONING capability is not present\n)\n\n# Get validated parameters with explicit handling of unsupported features\nvalidated_params = parameters.validate_parameters(\n    capabilities={ModelCapability.STREAMING, ModelCapability.FUNCTION_CALLING},\n    temperature=0.7,\n    include_reasoning=True,  # Will raise UnsupportedParameterError if REASONING not available\n    drop_unsupported=False   # Default is True which silently drops unsupported parameters\n)\n</code></pre> <p>Capability-Dependent Parameters</p> <p>Some parameters are only available when specific capabilities are enabled: - <code>include_reasoning</code>: Requires <code>REASONING</code> capability - <code>reasoning_effort</code>: Requires <code>REASONING</code> capability - Function calling parameters: Require <code>FUNCTION_CALLING</code> capability</p> <p>When a capability-dependent parameter is used without the required capability, it will be either: 1. Dropped silently (if <code>drop_unsupported=True</code>) 2. Raise an <code>UnsupportedParameterError</code> (if <code>drop_unsupported=False</code>)</p>"},{"location":"configs/constraints/","title":"Model Constraints","text":"<p>Model constraints define the technical limitations and boundaries of a model's operation. These constraints are crucial for managing token usage and ensuring requests stay within model capabilities.</p>"},{"location":"configs/constraints/#available-constraints","title":"Available Constraints","text":"Constraint Type Description <code>context_window</code> int Maximum total length of context (input + output) in tokens <code>max_input_tokens</code> int Maximum number of tokens allowed in input <code>max_output_tokens</code> int Maximum number of tokens allowed in output <p>All constraints must be positive integers (&gt;0).</p>"},{"location":"configs/constraints/#usage-example","title":"Usage Example","text":"<pre><code>from no_llm.config.model import ModelConstraints\n\nconstraints = ModelConstraints(\n    context_window=8192,      # 8K context window\n    max_input_tokens=7000,    # Maximum input size\n    max_output_tokens=4000    # Maximum output size\n)\n</code></pre>"},{"location":"configs/constraints/#input-size-estimation","title":"Input Size Estimation","text":"<p>The constraints system provides a quick way to estimate if text might exceed the input token limit:</p> <pre><code># Quick check if text might be too long\nlong_text = \"...\" # Your input text\nif constraints.estimate_exceeds_input_limit(long_text):\n    print(\"Warning: Text likely exceeds model's input token limit\")\n</code></pre> <p>Estimation Accuracy</p> <p>The estimation uses a simple heuristic of 4 characters per token. While this provides a quick check, actual token counts may vary based on the specific tokenizer used by the model.</p>"},{"location":"configs/inheritance/","title":"Configuration Inheritance","text":"<p>no_llm supports inheritance for model configurations, allowing you to create custom model configurations with specialized parameters and behaviors.</p>"},{"location":"configs/inheritance/#creating-custom-configurations","title":"Creating Custom Configurations","text":"<p>To create a custom model configuration, inherit from <code>ModelConfiguration</code> and optionally define custom parameters:</p> <pre><code>from no_llm.config import (\n    ModelConfiguration,\n    ConfigurableModelParameters,\n    ModelIdentity,\n    ModelMode,\n    ParameterValue,\n    ParameterVariant,\n    RangeValidation\n)\n\nclass CustomModelConfiguration(ModelConfiguration):\n    \"\"\"Custom model configuration with specific parameters\"\"\"\n\n    # Define custom parameters class for YAML serialization\n    class Parameters(ConfigurableModelParameters):\n        temperature: ParameterValue[float] = Field(\n            default_factory=lambda: ParameterValue(\n                variant=ParameterVariant.VARIABLE,\n                value=0.0,\n                validation_rule=RangeValidation(min_value=0.0, max_value=2.0)\n            )\n        )\n        top_p: ParameterValue[float] = Field(\n            default_factory=lambda: ParameterValue(\n                variant=ParameterVariant.FIXED,\n                value=1.0\n            )\n        )\n        # Mark unsupported parameters\n        frequency_penalty: ParameterValue[float] = Field(\n            default_factory=lambda: ParameterValue(\n                variant=ParameterVariant.UNSUPPORTED,\n                value=None\n            )\n        )\n\n    # Define model configuration\n    identity: ModelIdentity = ModelIdentity(\n        id=\"custom-model\",\n        name=\"Custom Model\",\n        version=\"1.0\",\n        description=\"Custom model configuration\",\n        creator=\"Your Organization\"\n    )\n    mode: ModelMode = ModelMode.CHAT\n    parameters: ConfigurableModelParameters = Field(default_factory=Parameters)\n</code></pre>"},{"location":"configs/inheritance/#yaml-serialization","title":"YAML Serialization","text":"<p>The custom Parameters class enables YAML configuration:</p> <pre><code>parameters:\n  temperature: 0.7  # Variable parameter\n  top_p: \n    variant: fixed\n    value: 1.0\n  frequency_penalty: unsupported\n</code></pre> <p>Parameter Class Inheritance</p> <p>Inheriting from <code>ConfigurableModelParameters</code> ensures your custom parameters can be: - Serialized to/from YAML - Validated properly - Used with the standard parameter system</p>"},{"location":"configs/inheritance/#real-world-example","title":"Real-World Example","text":"<p>Here's a simplified version of the Claude 3.5 Haiku configuration:</p> <pre><code>class Claude35HaikuConfiguration(ModelConfiguration):\n    class Parameters(ConfigurableModelParameters):\n        temperature: ParameterValue[float] = Field(\n            default_factory=lambda: ParameterValue(\n                variant=ParameterVariant.VARIABLE,\n                value=0.0,\n                validation_rule=RangeValidation(min_value=0.0, max_value=2.0)\n            )\n        )\n        top_k: ParameterValue[int] = Field(\n            default_factory=lambda: ParameterValue(\n                variant=ParameterVariant.FIXED,\n                value=40\n            )\n        )\n\n    identity: ModelIdentity = ModelIdentity(\n        id=\"claude-3.5-haiku\",\n        name=\"Claude 3.5 Haiku\",\n        version=\"2024.02\",\n        description=\"Fast and compact model for instant responses\",\n        creator=\"Anthropic\"\n    )\n\n    mode: ModelMode = ModelMode.CHAT\n    capabilities: set[ModelCapability] = {\n        ModelCapability.STREAMING,\n        ModelCapability.FUNCTION_CALLING,\n        ModelCapability.VISION\n    }\n\n    parameters: ConfigurableModelParameters = Field(default_factory=Parameters)\n</code></pre>"},{"location":"configs/mode/","title":"Model Modes","text":"<p>Model modes define the primary function of a model in no_llm. Each mode represents a specific type of task the model is designed to perform.</p> <p>Limited Mode Support</p> <p>Currently, no_llm primarily supports <code>CHAT</code> mode and chat-based models. Support for other modes (completion, embedding, image generation, audio) is under development and will be available in future releases.</p>"},{"location":"configs/mode/#available-modes","title":"Available Modes","text":"Mode Description Common Use Cases <code>CHAT</code> Interactive chat completion - Conversational AI- Virtual assistants- Interactive Q&amp;A <code>COMPLETION</code> Text completion - Text generation- Content creation- Code completion <code>EMBEDDING</code> Vector embedding generation - Semantic search- Document similarity- Text clustering <code>IMAGE_GENERATION</code> Image creation from text - Art generation- Design mockups- Visual content creation <code>AUDIO_TRANSCRIPTION</code> Speech-to-text conversion - Meeting transcription- Subtitle generation- Voice notes to text <code>AUDIO_SPEECH</code> Text-to-speech synthesis - Audio content creation- Accessibility features- Voice assistants"},{"location":"configs/mode/#listing-models-by-mode","title":"Listing Models by Mode","text":"<p>You can list all models supporting a specific mode using the registry:</p> <pre><code>from no_llm.config.enums import ModelMode\nfrom no_llm.registry import ModelRegistry\n\nregistry = ModelRegistry()\n\n# List all chat models\nchat_models = list(registry.list_models(mode=ModelMode.CHAT))\n\n# List chat models from a specific provider\nopenai_chat_models = list(registry.list_models(\n    mode=ModelMode.CHAT,\n    provider=\"openai\"\n))\n</code></pre>"},{"location":"configs/model_identity/","title":"Model Identity","text":"<p>Model identity provides essential identification and descriptive information for each model in no_llm.</p>"},{"location":"configs/model_identity/#identity-fields","title":"Identity Fields","text":"Field Type Description <code>id</code> str Unique identifier for the model <code>name</code> str Human-readable display name <code>version</code> str Model version identifier <code>description</code> str Detailed description of the model <code>creator</code> str Organization or entity that created the model <code>model_api_name</code> str | None Optional provider-specific API name"},{"location":"configs/model_identity/#usage-example","title":"Usage Example","text":"<pre><code>from no_llm.config.model import ModelIdentity\n\nidentity = ModelIdentity(\n    id=\"gpt-4-turbo\",\n    name=\"GPT-4 Turbo\",\n    version=\"1.0\",\n    description=\"Advanced language model with improved performance\",\n    creator=\"OpenAI\",\n    model_api_name=\"gpt-4-0125-preview\"  # Provider-specific name if different\n)\n</code></pre> <p>Model ID vs API Name</p> <p>The <code>id</code> field is no_llm's internal identifier, while <code>model_api_name</code> is used when the provider's API requires a different name. If <code>model_api_name</code> is not set, the <code>id</code> is used for API calls.</p> <p>Versioning</p> <p>Use semantic versioning in the <code>version</code> field to track model updates and ensure compatibility: <pre><code>identity = ModelIdentity(\n    id=\"gpt-4\",\n    version=\"1.2.3\",  # major.minor.patch\n    # ... other fields ...\n)\n</code></pre></p>"},{"location":"configs/overview/","title":"Configuration Overview","text":"<p>The no_llm configuration system provides a comprehensive way to define and manage model configurations. This overview will help you understand how the different components work together.</p>"},{"location":"configs/overview/#core-components","title":"Core Components","text":"<p>The configuration system consists of several key components:</p> <ul> <li>Model Identity: Basic model information like ID, name, and version</li> <li>Model Mode: Primary function of the model (chat, completion, embedding, etc.)</li> <li>Capabilities: Features supported by the model</li> <li>Constraints: Technical limitations like context window and token limits</li> <li>Properties: Performance and quality metrics</li> <li>Privacy: Compliance and data protection levels</li> <li>Pricing: Cost calculation configuration</li> </ul>"},{"location":"configs/overview/#built-in-models","title":"Built-in Models","text":"<p>no_llm includes configurations for popular models:</p>"},{"location":"configs/overview/#anthropic-models","title":"Anthropic Models","text":"<ul> <li>Claude 3 (Opus, Sonnet, Haiku)</li> <li>Claude 3.5 (Sonnet, Haiku)</li> <li>Claude 3.7 Sonnet</li> </ul>"},{"location":"configs/overview/#google-models","title":"Google Models","text":"<ul> <li>Gemini 1.5 (Pro, Flash)</li> <li>Gemini 2.0 (Pro, Flash, Flash Lite, Flash Thinking)</li> <li>Gemini 2.5 Pro</li> </ul>"},{"location":"configs/overview/#openai-models","title":"OpenAI Models","text":"<ul> <li>GPT-4 (Base, O, O Mini)</li> <li>GPT-3.5 Turbo</li> <li>O1/O3 Mini</li> </ul>"},{"location":"configs/overview/#other-providers","title":"Other Providers","text":"<ul> <li>DeepSeek (Chat, Reasoner, R1 Llama 70B)</li> <li>Llama 3 (405B, 70B)</li> <li>Mistral (Large, Nemo)</li> <li>Groq Mixtral</li> <li>Perplexity Sonar (Large, Small)</li> </ul>"},{"location":"configs/overview/#model-configuration-api","title":"Model Configuration API","text":"<pre><code>from no_llm.config.model import ModelConfiguration\nfrom no_llm.config.enums import ModelMode, ModelCapability\n\nmodel = ModelConfiguration(\n    identity=ModelIdentity(id=\"gpt-4\", name=\"GPT-4\", version=\"1.0.0\"),\n    mode=ModelMode.CHAT,\n    capabilities={ModelCapability.STREAMING},\n    providers=[OpenAIProvider(), AzureProvider()]  # Multiple providers supported\n)\n\n\n# Parameter Management\nmodel.set_parameters(ModelParameters(temperature=0.7))\nparams = model.get_parameters()  # Get current parameters\nnew_model = model.from_parameters(temperature=0.8)  # Create new config with parameters\n\n# Capability Checking\nmodel.check_capabilities({ModelCapability.STREAMING})  # Returns bool\nmodel.assert_capabilities({ModelCapability.STREAMING})  # Raises if missing\n\n# Cost Calculation\ninput_cost, output_cost = model.calculate_cost(\n    input_tokens=1000,\n    output_tokens=500\n)\n</code></pre>"},{"location":"configs/overview/#yaml-configuration","title":"YAML Configuration","text":"<p>Models can also be configured using YAML files:</p> <pre><code>identity:\n  id: gpt-4\n  name: GPT-4\n  version: 1.0.0\n\nmode: chat\ncapabilities: \n  - streaming\n  - function_calling\n\nproviders:\n  - type: openai\n    api_key: ${OPENAI_API_KEY}\n  - type: azure\n    api_key: ${AZURE_API_KEY}\n    deployment: gpt4\n\nconstraints:\n  context_window: 8192\n  max_input_tokens: 7000\n  max_output_tokens: 4000\n</code></pre>"},{"location":"configs/overview/#provider-iteration","title":"Provider Iteration","text":"<p>Models support iterating through providers and their variants (e.g., different locations for cloud providers):</p> <pre><code># Iterate through all providers and their variants\nfor provider in model.iter():\n    try:\n        response = call_model_with_provider(provider)\n        break  # Success, stop trying other providers\n    except Exception:\n        continue  # Try next provider\n</code></pre> <p>For cloud providers like Vertex AI, each location becomes a variant:</p> <pre><code>vertex_provider = VertexProvider(\n    project_id=\"my-project\",\n    locations=[\"us-central1\", \"europe-west1\"]\n)\n\n# Will yield a provider instance for each location\nfor provider in vertex_provider.iter():\n    print(provider.current)  # Access current location\n</code></pre> <p>See the specific component documentation for detailed information about each configuration aspect.</p>"},{"location":"configs/pricing/","title":"Model Pricing","text":"<p>no_llm supports both token-based and character-based pricing models for LLM usage calculations.</p>"},{"location":"configs/pricing/#pricing-models","title":"Pricing Models","text":""},{"location":"configs/pricing/#token-based-pricing","title":"Token-Based Pricing","text":"<p>Token-based pricing is the most common model, where costs are calculated per thousand tokens:</p> <pre><code>from no_llm.config.metadata import TokenPrices\n\npricing = TokenPrices(\n    input_price_per_1k=0.01,  # $0.01 per 1000 input tokens\n    output_price_per_1k=0.03   # $0.03 per 1000 output tokens\n)\n</code></pre>"},{"location":"configs/pricing/#character-based-pricing","title":"Character-Based Pricing","text":"<p>Some models use character-based pricing instead:</p> <pre><code>from no_llm.config.metadata import CharacterPrices\n\npricing = CharacterPrices(\n    input_price_per_1k=0.001,  # $0.001 per 1000 input characters\n    output_price_per_1k=0.002  # $0.002 per 1000 output characters\n)\n</code></pre>"},{"location":"configs/pricing/#cost-calculation","title":"Cost Calculation","text":"<p>You can calculate costs using either the model configuration or pricing models directly:</p> <pre><code># Using ModelConfiguration\nmodel_config = registry.get_model(\"gpt-4\")\ninput_cost, output_cost = model_config.calculate_cost(\n    input_tokens=1000,\n    output_tokens=500\n)\n\n# Using pricing models directly\npricing = ModelPricing(\n    token_prices=TokenPrices(\n        input_price_per_1k=0.01,\n        output_price_per_1k=0.03\n    )\n)\ninput_cost, output_cost = pricing.calculate_cost(\n    input_size=1000,    # tokens or characters depending on pricing type\n    output_size=500\n)\n</code></pre> <p>Pricing Configuration</p> <p>Models must have either token-based or character-based pricing configured. Attempting to configure both or neither will raise an <code>InvalidPricingConfigError</code>.</p> <p>Token Counting Limitations</p> <p>The current version has some limitations in token counting: - Reasoning steps tokens are not counted separately - Cached response tokens are not tracked</p> <p>These features will be added in future releases.</p>"},{"location":"configs/privacy/","title":"Model Privacy","text":"<p>no_llm supports various privacy levels for models, allowing you to choose the appropriate level of data protection for your use case.</p>"},{"location":"configs/privacy/#privacy-levels","title":"Privacy Levels","text":"Level Description <code>BASIC</code> Standard privacy level with default provider protections. Data may be used for model improvements. Suitable for non-sensitive information. <code>HIPAA</code> HIPAA-compliant processing for healthcare data. Includes strict access controls, audit trails, and data encryption. <code>GDPR</code> Compliant with EU General Data Protection Regulation. Ensures data sovereignty, processing limitations, and user rights protection. <code>FEDRAMP</code> Federal Risk and Authorization Management Program certified. Meets US government security standards for cloud services. <code>SOC2</code> Service Organization Control 2 certified. Ensures security, availability, processing integrity, and data confidentiality."},{"location":"configs/privacy/#filtering-models-by-privacy-level","title":"Filtering Models by Privacy Level","text":"<p>You can list models that match specific privacy requirements using the registry:</p> <pre><code>from no_llm.config.metadata import PrivacyLevel\nfrom no_llm.registry import ModelRegistry, SetFilter\n\nregistry = ModelRegistry()\n\n# List models with any of the specified privacy levels\nhipaa_or_gdpr_models = list(registry.list_models(\n    privacy_levels={PrivacyLevel.HIPAA, PrivacyLevel.GDPR}\n))\n\n# List models that have ALL specified privacy levels\nfully_compliant_models = list(registry.list_models(\n    privacy_levels=SetFilter(\n        values={PrivacyLevel.HIPAA, PrivacyLevel.GDPR},\n        mode=\"all\"\n    )\n))\n</code></pre> <p>Multiple Privacy Levels</p> <p>Models can support multiple privacy levels simultaneously. For example, a model might be both HIPAA and GDPR compliant.</p> <p>Privacy Verification</p> <p>Privacy levels are based on provider claims and certifications. Always verify the actual compliance requirements for your specific use case.</p>"},{"location":"configs/properties/","title":"Model Properties","text":"<p>Model properties provide informational metrics about model performance and quality. These properties are intended for display purposes to help users make informed decisions about model selection.</p> <p>Display Only</p> <p>Properties are informational metrics and do not affect model behavior. They are designed to help users understand and compare models.</p>"},{"location":"configs/properties/#speed-properties","title":"Speed Properties","text":"<p>Speed properties indicate the model's processing performance:</p> <pre><code>from no_llm.config.properties import SpeedProperties\n\nspeed = SpeedProperties(\n    score=1000.0,        # Tokens per second\n    label=\"Fast\",        # Quick reference label\n    description=\"Optimized for real-time applications\"\n)\n</code></pre>"},{"location":"configs/properties/#quality-properties","title":"Quality Properties","text":"<p>Quality properties reflect the model's output quality on a standardized scale:</p> <pre><code>from no_llm.config.properties import QualityProperties\n\nquality = QualityProperties(\n    score=95.0,          # Score from 0-100\n    label=\"High\",        # Quality tier\n    description=\"State-of-the-art performance on standard benchmarks\"\n)\n</code></pre> <p>Custom Properties</p> <p>You can extend the properties system by inheriting from the base Pydantic models: <pre><code>from no_llm.config.properties import ModelProperties\nfrom pydantic import BaseModel, Field\n\nclass CustomMetrics(BaseModel):\n    accuracy: float = Field(description=\"Model accuracy score\")\n    latency: float = Field(description=\"Average response time\")\n\nclass ExtendedProperties(ModelProperties):\n    metrics: CustomMetrics\n</code></pre></p>"},{"location":"integrations/pydantic_ai/","title":"Pydantic AI Integration","text":"<p>no_llm provides seamless integration with Pydantic AI, allowing you to use any no_llm model through the Pydantic AI interface.</p>"},{"location":"integrations/pydantic_ai/#installation","title":"Installation","text":"<pre><code>pip install \"no_llm[pydantic-ai]\"\n</code></pre>"},{"location":"integrations/pydantic_ai/#usage","title":"Usage","text":"<p>The integration provides a <code>NoLLMModel</code> class that wraps no_llm models for use with Pydantic AI:</p> <pre><code>from no_llm.integrations.pydantic_ai import NoLLMModel\nfrom no_llm.registry import ModelRegistry\nfrom pydantic_ai import Agent\nfrom pydantic_ai.settings import ModelSettings\n\n# Get models from registry\nregistry = ModelRegistry()\nmodels = list(registry.list_models(provider=\"openai\"))\n\n# Create NoLLMModel with fallbacks\nmodel = NoLLMModel(*models)\n\n# Use with Pydantic AI\nagent = Agent(model, model_settings=ModelSettings(temperature=0.7))\nresult = agent.run_sync(\"What is the capital of France?\")\n</code></pre>"},{"location":"integrations/pydantic_ai/#features","title":"Features","text":"<ul> <li>Model Fallbacks: Automatically tries alternative models if the primary model fails</li> <li>Parameter Validation: Validates and merges parameters from both no_llm and Pydantic AI settings</li> <li>Provider Support: Works with all no_llm supported providers including:</li> <li>OpenAI</li> <li>Anthropic</li> <li>Google Vertex AI</li> <li>Mistral</li> <li>Groq</li> <li>And more</li> </ul>"},{"location":"integrations/pydantic_ai/#model-settings","title":"Model Settings","text":"<p>The integration merges model settings from both no_llm and Pydantic AI:</p> <pre><code># no/llm parameters are merged with Pydantic AI settings, and validated with no/llm\nagent = Agent(\n    model,\n    model_settings=ModelSettings(\n        temperature=0.7,\n        top_p=0.9,\n        max_tokens=1000\n    )\n)\n</code></pre>"},{"location":"parameters/model_parameters/","title":"Model Parameters","text":"<p>no_llm provides two parameter classes: <code>ModelParameters</code> for runtime use and <code>ConfigurableModelParameters</code> for model configuration.</p> <p>Parameters vs Configurable Parameters</p> <ul> <li><code>ModelParameters</code>: Used at runtime for parameter passing, with simple value validation</li> <li><code>ConfigurableModelParameters</code>: Used in model configurations, with full validation rules, variants, and capability checks</li> </ul>"},{"location":"parameters/model_parameters/#supported-parameters","title":"Supported Parameters","text":""},{"location":"parameters/model_parameters/#sampling-parameters","title":"Sampling Parameters","text":"Parameter Type Description Default <code>temperature</code> float Controls randomness in generation 1.0 <code>top_p</code> float Nucleus sampling threshold 1.0 <code>top_k</code> int | None Top-k sampling threshold None"},{"location":"parameters/model_parameters/#penalty-parameters","title":"Penalty Parameters","text":"Parameter Type Description Default <code>frequency_penalty</code> float | None Penalty for token frequency 0.0 <code>presence_penalty</code> float | None Penalty for token presence 0.0 <code>logit_bias</code> dict[str, float] | None Token biasing dictionary None"},{"location":"parameters/model_parameters/#output-parameters","title":"Output Parameters","text":"Parameter Type Description Default <code>max_tokens</code> int | None Maximum tokens to generate None <code>stop</code> list[str] | None Stop sequences None <code>logprobs</code> int | None Number of logprobs to return None <code>top_logprobs</code> int | None Most likely tokens to return None <code>seed</code> int | None Random seed for reproducibility None"},{"location":"parameters/model_parameters/#request-parameters","title":"Request Parameters","text":"Parameter Type Description Default <code>timeout</code> float | None Request timeout in seconds None"},{"location":"parameters/model_parameters/#reasoning-parameters","title":"Reasoning Parameters","text":"Parameter Type Description Default Required Capability <code>include_reasoning</code> bool | None Include reasoning steps False <code>REASONING</code> <code>reasoning_effort</code> \"low\"|\"medium\"|\"high\" | None Reasoning level None <code>REASONING</code>"},{"location":"parameters/model_parameters/#usage-example","title":"Usage Example","text":"<pre><code># Runtime parameter passing\nfrom no_llm.config.parameters import ModelParameters\n\nparams = ModelParameters(\n    temperature=0.7,\n    max_tokens=100,\n    include_reasoning=True\n)\n\n# Model configuration\nfrom no_llm.config.parameters import ConfigurableModelParameters\n\nconfig_params = ConfigurableModelParameters(\n    temperature=ParameterValue(\n        variant=ParameterVariant.VARIABLE,\n        value=0.7,\n        validation_rule=RangeValidation(min_value=0.0, max_value=2.0)\n    )\n)\n</code></pre> <p>NOT_GIVEN vs None</p> <p>In <code>ModelParameters</code>, unset parameters use the special value <code>'NOT_GIVEN'</code> instead of <code>None</code>. This allows distinguishing between explicitly setting a parameter to <code>None</code> and not setting it at all.</p>"},{"location":"parameters/model_parameters/#parameter-conversion-flow","title":"Parameter Conversion Flow","text":"<p>ConfigurableModelParameters can be converted to ModelParameters through validation:</p> <pre><code># Model configuration with validation rules\nconfig_params = ConfigurableModelParameters(\n    temperature=ParameterValue(\n        variant=ParameterVariant.VARIABLE,\n        value=0.7,\n        validation_rule=RangeValidation(min_value=0.0, max_value=2.0)\n    )\n)\n\n# Get validated runtime parameters\nvalidated_params = config_params.validate_parameters(\n    capabilities={ModelCapability.STREAMING},\n    temperature=0.8  # Will be validated against rules\n)\n\n# Create ModelParameters from validated values\nmodel_params = ModelParameters(**validated_params)\n</code></pre> <p>Extensibility</p> <p>The parameter system can be extended through inheritance to support additional parameters Check out the Parameter Customization section for more information.</p>"},{"location":"parameters/overview/","title":"Parameter System Overview","text":"<p>The no_llm parameter system provides a robust way to configure, validate, and manage model parameters across different LLM providers.</p>"},{"location":"parameters/overview/#core-components","title":"Core Components","text":""},{"location":"parameters/overview/#parameter-types","title":"Parameter Types","text":"<ul> <li>Variable: Modifiable values with validation rules</li> <li>Fixed: Immutable values</li> <li>Unsupported: Parameters not supported by the model</li> </ul> <p>Learn more about parameter variants Learn more about parameter classes</p>"},{"location":"parameters/overview/#validation","title":"Validation","text":"<ul> <li>Range constraints</li> <li>Enum values</li> <li>Capability requirements</li> <li>Fixed value protection</li> </ul> <p>Learn more about validation</p>"},{"location":"parameters/overview/#quick-example","title":"Quick Example","text":"<pre><code>from no_llm.config.model import ModelConfiguration\nfrom no_llm.config.enums import ModelCapability\n\n# Configure model parameters\nmodel = ModelConfiguration(\n    parameters=ConfigurableModelParameters(\n        temperature=0.7,           # Variable parameter\n        top_p={'fixed': 0.9},     # Fixed parameter\n        include_reasoning=True     # Capability-dependent parameter\n    )\n)\n\n# Validate and get runtime parameters\nparams = model.parameters.validate_parameters(\n    capabilities={ModelCapability.REASONING},\n    temperature=0.8  # Will be validated\n)\n</code></pre> <p>Best Practices</p> <ul> <li>Use <code>ConfigurableModelParameters</code> for model definitions</li> <li>Use <code>ModelParameters</code> for runtime parameter passing</li> <li>Always validate parameters against model capabilities</li> </ul> <p>See the specific documentation sections for detailed information about each aspect of the parameter system.</p>"},{"location":"parameters/parameter_value/","title":"Parameter Value API","text":"<p>The <code>ParameterValue</code> class provides a flexible way to define and validate model parameters.</p> <p>Custom Validation vs Pydantic</p> <p>While no_llm uses Pydantic for model definitions, parameter validation uses a custom system. This allows parameter configurations to be easily serialized to YAML/JSON and used in any environment, not just Python. The validation rules become part of the configuration data itself, making it portable and platform-independent.</p>"},{"location":"parameters/parameter_value/#core-methods","title":"Core Methods","text":""},{"location":"parameters/parameter_value/#creation","title":"Creation","text":"<pre><code>from no_llm.config.parameters import ParameterValue, ParameterVariant\nfrom no_llm.config.enums import ModelCapability\n\n# Standard creation\nparam = ParameterValue(\n    variant=ParameterVariant.VARIABLE,\n    value=0.7,\n    validation_rule=RangeValidation(min_value=0.0, max_value=2.0)\n)\n\n# Helper for variable parameters\nparam = ParameterValue.create_variable(\n    value=True,\n    required_capability=ModelCapability.REASONING\n)\n</code></pre>"},{"location":"parameters/parameter_value/#value-access-and-validation","title":"Value Access and Validation","text":"<pre><code># Get value\nvalue = param.get()  # Returns value or 'UNSUPPORTED'\n\n# Validate new value\nparam.validate_new_value(0.8, \"temperature\")\n\n# Check capabilities\nparam.check_capability({ModelCapability.STREAMING})\n\n# Check variant type\nparam.is_variable()    # True if variable\nparam.is_fixed()       # True if fixed\nparam.is_unsupported() # True if unsupported\n</code></pre>"},{"location":"parameters/parameter_value/#validation-rules","title":"Validation Rules","text":"<pre><code># Range validation\nRangeValidation(min_value=0.0, max_value=2.0)\n\n# Enum validation\nEnumValidation(allowed_values=[\"low\", \"medium\", \"high\"])\n</code></pre> <p>Parameter Value Behavior</p> <ul> <li>Fixed parameters cannot be modified after creation</li> <li>Unsupported parameters always return 'UNSUPPORTED'</li> <li>Variable parameters require validation rule checks</li> </ul>"},{"location":"parameters/validation/","title":"Parameter Validation","text":"<p>no_llm provides a robust parameter validation system that ensures model parameters are used correctly and safely.</p>"},{"location":"parameters/validation/#validation-modes","title":"Validation Modes","text":"<p>no_llm supports three validation modes for handling invalid parameter values:</p> Mode Description <code>ERROR</code> Raises an exception when validation fails <code>WARN</code> Logs a warning and keeps the original parameter value <code>CLAMP</code> Clamps values to the nearest valid value within range <pre><code>from no_llm.settings import settings as no_llm_settings\nfrom no_llm.settings import ValidationMode\n\n# Set validation mode\nno_llm_settings.validation_mode = ValidationMode.CLAMP\n\n# Now out-of-range values will be clamped instead of raising errors\nparameters.validate_parameters(temperature=2.5)  # Will be clamped to 2.0\n</code></pre>"},{"location":"parameters/validation/#types-of-validation","title":"Types of Validation","text":""},{"location":"parameters/validation/#range-validation","title":"Range Validation","text":"<p>Ensures numeric parameters stay within defined bounds:</p> <pre><code># Will raise InvalidRangeError in ERROR mode\n# Will clamp to 2.0 in CLAMP mode\nparameters.validate_parameters(temperature=2.5)\n\n# Will raise InvalidRangeError in ERROR mode\n# Will clamp to 0.0 in CLAMP mode\nparameters.validate_parameters(temperature=-0.5)\n</code></pre>"},{"location":"parameters/validation/#enum-validation","title":"Enum Validation","text":"<p>Ensures parameters only take predefined values:</p> <pre><code># Will raise InvalidEnumError\nparameters.validate_parameters(\n    reasoning_effort=\"very_high\"  # Only low/medium/high allowed\n)\n</code></pre>"},{"location":"parameters/validation/#capability-based-validation","title":"Capability-Based Validation","text":"<p>Some parameters require specific model capabilities:</p> <pre><code># Check if parameters are valid for model capabilities\nvalidated_params = parameters.validate_parameters(\n    capabilities={ModelCapability.STREAMING},\n    include_reasoning=True  # Requires REASONING capability\n)\n</code></pre>"},{"location":"parameters/validation/#handling-unsupported-parameters","title":"Handling Unsupported Parameters","text":"<p>When validating parameters, you can control how unsupported parameters are handled:</p> <pre><code># Silently drop unsupported parameters (default)\nvalidated_params = parameters.validate_parameters(\n    capabilities={ModelCapability.STREAMING},\n    include_reasoning=True,  # Will be dropped if REASONING not supported\n    drop_unsupported=True\n)\n\n# Raise error for unsupported parameters\ntry:\n    validated_params = parameters.validate_parameters(\n        capabilities={ModelCapability.STREAMING},\n        include_reasoning=True,\n        drop_unsupported=False  # Will raise UnsupportedParameterError\n    )\nexcept UnsupportedParameterError as e:\n    print(f\"Parameter {e.param_name} requires capability: {e.required_capability}\")\n</code></pre> <p>Validation Best Practices</p> <ul> <li>Use <code>ERROR</code> mode during development to catch issues early</li> <li>Use <code>CLAMP</code> mode in production for better user experience</li> <li>Always check model capabilities before setting parameters</li> <li>Handle unsupported parameters explicitly in critical code paths</li> </ul>"},{"location":"parameters/variant/","title":"Parameter Variants","text":"<p>Parameters in no_llm can exist in three variants that define their behavior and configurability.</p>"},{"location":"parameters/variant/#available-variants","title":"Available Variants","text":"Variant Description <code>FIXED</code> Parameter value cannot be changed <code>VARIABLE</code> Parameter value can be modified within constraints <code>UNSUPPORTED</code> Parameter is not supported by the model"},{"location":"parameters/variant/#python-usage","title":"Python Usage","text":"<pre><code>from no_llm.config.parameters import ParameterValue, ParameterVariant\n\n# Fixed parameter\ntemp_fixed = ParameterValue(\n    variant=ParameterVariant.FIXED,\n    value=0.7\n)\n\n# Variable parameter\ntemp_variable = ParameterValue(\n    variant=ParameterVariant.VARIABLE,\n    value=0.7\n)\n\n# Unsupported parameter\ntemp_unsupported = ParameterValue(\n    variant=ParameterVariant.UNSUPPORTED\n)\n</code></pre>"},{"location":"parameters/variant/#yaml-configuration","title":"YAML Configuration","text":"<p>Parameters can be configured in YAML using different formats:</p>"},{"location":"parameters/variant/#simple-format","title":"Simple Format","text":"<pre><code># Fixed value (shorthand)\ntemperature: 0.7\n\n# Unsupported parameter (shorthand)\nfrequency_penalty: unsupported\n</code></pre>"},{"location":"parameters/variant/#explicit-format","title":"Explicit Format","text":"<pre><code># Fixed parameter\ntemperature:\n  variant: fixed\n  value: 0.7\n\n# Variable parameter\ntop_p:\n  variant: variable\n  value: 0.9\n\n# Unsupported parameter\nlogprobs:\n  variant: unsupported\n</code></pre>"},{"location":"parameters/variant/#variable-with-validation","title":"Variable with Validation","text":"<pre><code># Variable with range validation\ntemperature:\n  variant: variable\n  value: 0.7\n  range: [0.0, 2.0]\n\n# Variable with enum validation\nreasoning_effort:\n  variant: variable\n  value: \"medium\"\n  values: [\"low\", \"medium\", \"high\"]\n</code></pre> <p>Choosing Variants</p> <ul> <li>Use <code>FIXED</code> for parameters that should never change</li> <li>Use <code>VARIABLE</code> for parameters that users can modify</li> <li>Use <code>UNSUPPORTED</code> to explicitly mark unavailable features</li> <li>Default to <code>VARIABLE</code> when in doubt</li> </ul>"},{"location":"providers/fallbacks/","title":"Provider Fallbacks","text":"<p>no_llm supports multiple levels of fallback to ensure reliable model access:</p>"},{"location":"providers/fallbacks/#data-center-fallback","title":"Data Center Fallback","text":"<p>Cloud providers can have multiple data centers configured for automatic failover:</p> <pre><code>from no_llm.providers import VertexProvider\n\nprovider = VertexProvider(\n    project_id=\"my-project\",\n    locations=[\"us-central1\", \"europe-west1\", \"asia-east1\"]\n)\n\n# Automatically tries each location\nfor variant in provider.iter():\n    try:\n        ...\n        break  # Success, stop trying other locations\n    except Exception:\n        continue  # Try next location\n</code></pre>"},{"location":"providers/fallbacks/#provider-fallback","title":"Provider Fallback","text":"<p>Models can be configured with multiple providers for service redundancy:</p> <pre><code>from no_llm.config import ModelConfiguration\nfrom no_llm.providers import OpenAIProvider, AnthropicProvider, AzureProvider\n\nmodel = ModelConfiguration(\n    providers=[\n        OpenAIProvider(api_key=\"$OPENAI_API_KEY\"),\n        AzureProvider(api_key=\"$AZURE_API_KEY\"),\n        AnthropicProvider(api_key=\"$ANTHROPIC_API_KEY\")\n    ]\n)\n\n# Try each provider in sequence\nfor provider in model.iter():\n    try:\n        ...\n        break  # Success, stop trying other providers\n    except Exception:\n        continue  # Try next provider\n</code></pre>"},{"location":"providers/fallbacks/#combined-fallback-strategy","title":"Combined Fallback Strategy","text":"<p>The iteration system combines both levels of fallback:</p> <pre><code>model = ModelConfiguration(\n    providers=[\n        VertexProvider(\n            project_id=\"my-project\",\n            locations=[\"us-central1\", \"europe-west1\"]\n        ),\n        OpenAIProvider(api_key=\"$OPENAI_API_KEY\"),\n        AnthropicProvider(api_key=\"$ANTHROPIC_API_KEY\")\n    ]\n)\n\n# Tries each provider and their variants\nfor provider in model.iter():\n    try:\n        ...\n        break\n    except Exception:\n        continue\n</code></pre> <p>Fallback Order</p> <ul> <li>Providers are tried in the order they are configured</li> <li>Each provider's data centers are tried in the order specified</li> <li>Use <code>reset_provider_iteration()</code> to start over from the first provider</li> </ul> <p>Environment Validation</p> <p>Providers with missing environment variables (e.g., unset API keys) are automatically skipped during iteration.</p>"},{"location":"providers/overview/","title":"Providers","text":"<p>no_llm supports multiple LLM providers through a flexible provider system. Each provider can be configured with its own authentication and endpoint settings.</p>"},{"location":"providers/overview/#supported-providers","title":"Supported Providers","text":"Provider Type Required Configuration OpenAI <code>openai</code> <code>OPENAI_API_KEY</code> Anthropic <code>anthropic</code> <code>ANTHROPIC_API_KEY</code> Azure OpenAI <code>azure</code> <code>AZURE_API_KEY</code>, <code>AZURE_BASE_URL</code> Google Vertex AI <code>vertex</code> <code>VERTEX_PROJECT_ID</code> Mistral AI <code>mistral</code> <code>MISTRAL_API_KEY</code> Groq <code>groq</code> <code>GROQ_API_KEY</code> Perplexity <code>perplexity</code> <code>PERPLEXITY_API_KEY</code> DeepSeek <code>deepseek</code> <code>DEEPSEEK_API_KEY</code> Together AI <code>together</code> <code>TOGETHER_API_KEY</code> OpenRouter <code>openrouter</code> <code>OPENROUTER_API_KEY</code> Grok <code>grok</code> <code>GROK_API_KEY</code> Fireworks <code>fireworks</code> <code>FIREWORKS_API_KEY</code> AWS Bedrock <code>bedrock</code> <code>BEDROCK_REGION</code>"},{"location":"providers/overview/#environment-variables","title":"Environment Variables","text":"<p>no_llm uses environment variables for provider configuration. The <code>EnvVar</code> class provides a secure way to handle API keys and other sensitive information:</p> <pre><code>from no_llm.providers import OpenAIProvider\n\nprovider = OpenAIProvider(\n    api_key=\"$OPENAI_API_KEY\",  # Will load from environment\n    base_url=None  # Optional API endpoint override\n)\n</code></pre> <p>Environment Variable Format</p> <p>Environment variables must be prefixed with <code>$</code> in the configuration. The actual environment variable name will not include the <code>$</code>.</p>"},{"location":"providers/overview/#provider-configuration","title":"Provider Configuration","text":"<p>Providers can be configured in Python or YAML:</p> <pre><code>from no_llm.config import ModelConfiguration\nfrom no_llm.providers import OpenAIProvider, AnthropicProvider\n\n# Multiple providers for fallback\nmodel = ModelConfiguration(\n    providers=[\n        OpenAIProvider(api_key=\"$OPENAI_API_KEY\"),\n        AnthropicProvider(api_key=\"$ANTHROPIC_API_KEY\")\n    ]\n)\n\n\n```yaml\nproviders:\n  - type: openai\n    api_key: ${OPENAI_API_KEY}\n  - type: anthropic\n    api_key: ${ANTHROPIC_API_KEY}\n</code></pre>"},{"location":"providers/overview/#provider-features","title":"Provider Features","text":"<p>Each provider implementation includes: - Environment variable handling - API endpoint configuration - Parameter mapping - Error handling</p> <p>Provider Selection</p> <p>Models can have multiple providers configured for: - Fallback handling - Load balancing - Cost optimization - Geographic distribution</p> <p>See the specific provider documentation for detailed configuration options and features.</p>"},{"location":"providers/overview/#provider-iteration","title":"Provider Iteration","text":"<p>Providers support iteration over their variants (e.g., different locations for cloud providers):</p> <pre><code>for provider in model.iter():\n    try:\n        ...\n        break  # Success, stop trying other providers\n    except Exception:\n        continue  # Try next provider/variant\n</code></pre> <p>Each provider implements the <code>iter()</code> method to define its iteration behavior: - Base providers yield just themselves - Cloud providers yield variants for each location - Custom providers can implement their own iteration logic</p>"}]}